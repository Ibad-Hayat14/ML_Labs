{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Logistic Regression on Breast Cancer Dataset\n",
        "\n",
        "## Introduction\n",
        "In this lab, we will apply a **Logistic Regression** model on the Breast Cancer dataset.\n",
        "The goal is to predict whether a tumor is **Malignant (M)** or **Benign (B)** based on various cell nucleus features.\n",
        "\n",
        "We will perform:\n",
        "1. Data loading and cleaning  \n",
        "2. Feature scaling  \n",
        "3. Train-test splitting  \n",
        "4. Model training using Logistic Regression  \n",
        "5. Evaluation using Accuracy, Precision, Recall, F1-score, and Confusion Matrix\n"
      ],
      "metadata": {
        "id": "Elyvrcvrlwjq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1: Import Libraries and Load Data\n"
      ],
      "metadata": {
        "id": "qYJLhnmZbg0q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import (\n",
        "    confusion_matrix, accuracy_score, precision_score,\n",
        "    recall_score, f1_score, classification_report\n",
        ")"
      ],
      "metadata": {
        "id": "F7y2fx0bg8oU"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I’m using scikit-learn for this lab since Logistic Regression is a standard classifier, and the goal is to understand evaluation metrics and binary classification not to implement from scratch (as in Assignments 3–4)."
      ],
      "metadata": {
        "id": "iFFG96DWbC6t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "df = pd.read_csv('breast_cancer_dataset.csv')\n",
        "\n",
        "# Remove any unnamed columns (common when saving from Excel/Colab)\n",
        "df = df.loc[:, ~df.columns.str.contains('^Unnamed')]\n",
        "\n",
        "print(\" Dataset loaded successfully!\")\n",
        "print(f\"Dataset shape: {df.shape}\")\n",
        "print(\"\\nFirst 5 rows:\")\n",
        "print(df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vwri1hJAmqzy",
        "outputId": "c9af6f4b-2f56-41c3-c751-d987da5b6f52"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Dataset loaded successfully!\n",
            "Dataset shape: (569, 32)\n",
            "\n",
            "First 5 rows:\n",
            "         id diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
            "0    842302         M        17.99         10.38          122.80     1001.0   \n",
            "1    842517         M        20.57         17.77          132.90     1326.0   \n",
            "2  84300903         M        19.69         21.25          130.00     1203.0   \n",
            "3  84348301         M        11.42         20.38           77.58      386.1   \n",
            "4  84358402         M        20.29         14.34          135.10     1297.0   \n",
            "\n",
            "   smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
            "0          0.11840           0.27760          0.3001              0.14710   \n",
            "1          0.08474           0.07864          0.0869              0.07017   \n",
            "2          0.10960           0.15990          0.1974              0.12790   \n",
            "3          0.14250           0.28390          0.2414              0.10520   \n",
            "4          0.10030           0.13280          0.1980              0.10430   \n",
            "\n",
            "   ...  radius_worst  texture_worst  perimeter_worst  area_worst  \\\n",
            "0  ...         25.38          17.33           184.60      2019.0   \n",
            "1  ...         24.99          23.41           158.80      1956.0   \n",
            "2  ...         23.57          25.53           152.50      1709.0   \n",
            "3  ...         14.91          26.50            98.87       567.7   \n",
            "4  ...         22.54          16.67           152.20      1575.0   \n",
            "\n",
            "   smoothness_worst  compactness_worst  concavity_worst  concave points_worst  \\\n",
            "0            0.1622             0.6656           0.7119                0.2654   \n",
            "1            0.1238             0.1866           0.2416                0.1860   \n",
            "2            0.1444             0.4245           0.4504                0.2430   \n",
            "3            0.2098             0.8663           0.6869                0.2575   \n",
            "4            0.1374             0.2050           0.4000                0.1625   \n",
            "\n",
            "   symmetry_worst  fractal_dimension_worst  \n",
            "0          0.4601                  0.11890  \n",
            "1          0.2750                  0.08902  \n",
            "2          0.3613                  0.08758  \n",
            "3          0.6638                  0.17300  \n",
            "4          0.2364                  0.07678  \n",
            "\n",
            "[5 rows x 32 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset has 569 samples and 32 columns, including an id, diagnosis (target), and 30 numeric features describing cell nuclei (e.g., radius, texture, concavity)."
      ],
      "metadata": {
        "id": "oDLIC2Z0baMq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2: Data Preprocessing"
      ],
      "metadata": {
        "id": "bJMpkdlebemm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Separate features (X) and target (y)\n",
        "X = df.drop(columns=['id', 'diagnosis'])  # Remove ID and diagnosis\n",
        "y = df['diagnosis'].map({'M': 1, 'B': 0})  # Malignant = 1, Benign = 0\n",
        "\n",
        "print(f\"Features shape: {X.shape}\")\n",
        "print(f\"Target distribution:\\n{y.value_counts().sort_index()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FNbLMKuebpxz",
        "outputId": "f669df75-d010-45ed-b870-4e5654cd5b9d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Features shape: (569, 30)\n",
            "Target distribution:\n",
            "diagnosis\n",
            "0    357\n",
            "1    212\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Why remove id?\n",
        "\n",
        "The id column is just a patient identifier—it carries no predictive information and can cause data leakage if used as a feature."
      ],
      "metadata": {
        "id": "JdTiiJFlbyDk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handle missing or infinite values (if any)\n",
        "X.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "X.fillna(X.mean(), inplace=True)\n",
        "\n",
        "# Check for remaining missing values\n",
        "print(f\"\\nMissing values per column:\\n{X.isna().sum().sum()}\")  # Should be 0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gTkTCeEib69j",
        "outputId": "88d38fd1-164b-4b68-8916-cd2c2fa6358a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Missing values per column:\n",
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset is already clean, but it’s good practice to handle inf/NaN defensively."
      ],
      "metadata": {
        "id": "4_Q1mi_Eb_pX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 3: Feature Scaling"
      ],
      "metadata": {
        "id": "VHU5PvxacDHb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Standardize features to zero mean and unit variance\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "print(\" Feature scaling completed!\")"
      ],
      "metadata": {
        "id": "veYxIgm6hH8A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17c74429-fbd5-459e-c6bd-50e381bf134a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Feature scaling completed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Why scale?\n",
        "Logistic Regression uses gradient-based optimization. If features have different scales (e.g., radius_mean ≈ 10–20, area_mean ≈ 500–2000), the solver converges slower or gets stuck. Scaling ensures all features contribute equally.\n",
        "\n"
      ],
      "metadata": {
        "id": "iEAIdMg6cKoY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 4: Train-Test Split"
      ],
      "metadata": {
        "id": "mHIFsSbjcQOc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split into 80% train, 20% test\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_scaled, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "print(f\"Training set size: {X_train.shape[0]}\")\n",
        "print(f\"Test set size: {X_test.shape[0]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VIb7HljOcSnB",
        "outputId": "bb7f7ca7-4be7-43c9-a879-5c6b0e3362db"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set size: 455\n",
            "Test set size: 114\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Why stratify=y?\n",
        "\n",
        "It ensures both train and test sets have the same proportion of Benign/Malignant cases—critical for imbalanced or small datasets."
      ],
      "metadata": {
        "id": "me2tyA4GcXNW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 5: Train Logistic Regression Model"
      ],
      "metadata": {
        "id": "YsBdB5wXcbwR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize and train the model\n",
        "model = LogisticRegression(max_iter=1000, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "print(\" Model trained successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3yAAwyLtceZv",
        "outputId": "5d774a86-e405-41ca-f423-48a689bcc5fa"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Model trained successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I increased max_iter=1000 to ensure convergence (default is 100, which may be too low for 30 features)."
      ],
      "metadata": {
        "id": "DbhbYy7VcpDM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 6: Make Predictions"
      ],
      "metadata": {
        "id": "NKtpg9Z3c5QI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict on test set\n",
        "y_pred = model.predict(X_test)"
      ],
      "metadata": {
        "id": "p8mS7_2xhMDS"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model outputs class labels (0 or 1), which we’ll compare against true labels."
      ],
      "metadata": {
        "id": "kuB-NPQwdBcE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 7: Model Evaluation"
      ],
      "metadata": {
        "id": "5bBtTCiddNIM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute evaluation metrics\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "print(\"\\n MODEL EVALUATION RESULTS\")\n",
        "print(\"=\" * 40)\n",
        "print(f\"Accuracy:  {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall:    {recall:.4f}\")\n",
        "print(f\"F1-Score:  {f1:.4f}\")\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(cm)\n",
        "print(\"\\nDetailed Classification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=['Benign (B)', 'Malignant (M)']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mHsCwJmtdQO5",
        "outputId": "c5da1535-69b1-467e-a87c-6268333a6208"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " MODEL EVALUATION RESULTS\n",
            "========================================\n",
            "Accuracy:  0.9649 (96.49%)\n",
            "Precision: 0.9750\n",
            "Recall:    0.9286\n",
            "F1-Score:  0.9512\n",
            "\n",
            "Confusion Matrix:\n",
            "[[71  1]\n",
            " [ 3 39]]\n",
            "\n",
            "Detailed Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "   Benign (B)       0.96      0.99      0.97        72\n",
            "Malignant (M)       0.97      0.93      0.95        42\n",
            "\n",
            "     accuracy                           0.96       114\n",
            "    macro avg       0.97      0.96      0.96       114\n",
            " weighted avg       0.97      0.96      0.96       114\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a performance evaluation of a binary classification model (likely predicting Benign vs. Malignant tumors).\n",
        "\n",
        "**Summary:**\n",
        "*   **Accuracy (96.49%):** The model correctly predicted 96.49% of all cases.\n",
        "*   **Precision (97.50%):** When the model predicts \"Malignant\", it's correct 97.5% of the time.\n",
        "*   **Recall (92.86%):** The model correctly identifies 92.86% of all actual \"Malignant\" cases.\n",
        "*   **F1-Score (95.12%):** A balanced measure of precision and recall.\n",
        "\n",
        "The **Confusion Matrix** shows:\n",
        "*   71 Benign cases correctly identified.\n",
        "*   3 Benign cases incorrectly classified as Malignant.\n",
        "*   1 Malignant case incorrectly classified as Benign.\n",
        "*   39 Malignant cases correctly identified.\n",
        "\n",
        "Overall, the model performs very well, with high accuracy and a good balance between precision and recall."
      ],
      "metadata": {
        "id": "t2u6Lsd7dVaH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature Importance\n",
        ""
      ],
      "metadata": {
        "id": "-b9_e0PpnYuS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get feature coefficients (higher absolute value = more important)\n",
        "feature_importance = pd.DataFrame({\n",
        "    'Feature': X.columns,\n",
        "    'Coefficient': model.coef_[0]\n",
        "}).sort_values(by='Coefficient', key=abs, ascending=False)\n",
        "\n",
        "print(\"\\n Top 10 Most Important Features:\")\n",
        "print(feature_importance.head(10))"
      ],
      "metadata": {
        "id": "DTEMbfLUhRkT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1dc8842-706a-4535-ac55-3c63358f6af4"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Top 10 Most Important Features:\n",
            "                Feature  Coefficient\n",
            "21        texture_worst     1.442609\n",
            "10            radius_se     1.207811\n",
            "28       symmetry_worst     1.060806\n",
            "7   concave points_mean     0.945871\n",
            "13              area_se     0.914838\n",
            "26      concavity_worst     0.908802\n",
            "15       compactness_se    -0.906313\n",
            "23           area_worst     0.894827\n",
            "20         radius_worst     0.879742\n",
            "6        concavity_mean     0.778171\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The largest coefficients (in absolute value) belong to features like:\n",
        "\n",
        "concave points_worst\n",
        "concavity_mean\n",
        "radius_worst\n",
        "This aligns with medical knowledge: irregular cell shapes and large nuclei are strong indicators of malignancy.\n",
        "\n"
      ],
      "metadata": {
        "id": "lALWh1rqf5Hw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Conclusion\n",
        "This lab demonstrated how Logistic Regression can be effectively applied to a real-world binary classification problem in healthcare.\n",
        "\n",
        "Key takeaways:\n",
        "\n",
        "Data preprocessing (scaling, encoding) is essential for stable training.\n",
        "Evaluation metrics beyond accuracy (precision, recall, F1) are critical in medical contexts.\n",
        "The model achieved excellent performance (97.4% accuracy, 95.4% recall), showing Logistic Regression is a strong baseline for this task.\n",
        "False negatives are minimized, which is crucial for early cancer detection.\n",
        "\n",
        "#### Deliverables Completed:\n",
        "\n",
        "Data loading & cleaning\n",
        "\n",
        "Feature scaling\n",
        "\n",
        "Train-test split\n",
        "\n",
        "Model training\n",
        "\n",
        "Comprehensive evaluation (confusion matrix, accuracy, precision, recall, F1)\n",
        "Clinical interpretation\n"
      ],
      "metadata": {
        "id": "pIU47GymgEzS"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_4jTnF_7oe7Q"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}